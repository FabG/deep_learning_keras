{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Recurrent Neural Network, LSTM Sentiment Classification\n",
    "Source: Pan, Chao. Deep Learning With Python: Step By Step Guide With Keras and Pytorch (Kindle Locations 1799-1801). \n",
    "\n",
    "Task - Sentiment classification\n",
    "Ordering of words in a sentence is paramount to arriving at an appropriate estimation of users feelings on the subject matter. The task involves classifying users movie reviews into positive or negative categories. We would employ the IMDB movies review classification dataset for our binary classification task. \n",
    "The IMDB dataset contains 25,000 movie reviews each in the training and test set, making a total of 50,000 samples. We would use Keras as it bundles the dataset in a preprocessed form. Each review is already encoded as a sequence of word indices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Fab/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence \n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Embedding \n",
    "from keras.layers import LSTM \n",
    "from keras.datasets import imdb \n",
    "\n",
    "# Set hyperparameters \n",
    "max_features = 20000 # use 20,000 most common words in corpus \n",
    "maxlen = 80 # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset and print the number of samples. Note that on the first run the dataset is downloaded into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loading data...\n",
      "(25000, 'train sequences')\n",
      "(25000, 'test sequences')\n"
     ]
    }
   ],
   "source": [
    "print(' Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data( num_words = max_features) \n",
    "print( len( x_train), 'train sequences') \n",
    "print( len( x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure all reviews fed into the model contains 80 words, \n",
    "we truncate reviews greater than 80 words and pad reviews that are less than 80 words with dummy (0) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pad sequences (samples x time)\n",
      "(' x_train shape:', (25000, 80))\n",
      "(' x_test shape:', (25000, 80))\n"
     ]
    }
   ],
   "source": [
    "print(' Pad sequences (samples x time)') \n",
    "x_train = sequence.pad_sequences( x_train, maxlen = maxlen) \n",
    "x_test = sequence.pad_sequences( x_test, maxlen = maxlen) \n",
    "print(' x_train shape:', x_train.shape) \n",
    "print(' x_test shape:', x_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step involves defining our model. The model utilises Keras embedding layer, an LSTM layer with 128 units and finally a dense layer with one unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model definition\n",
    "model = Sequential()\n",
    "model.add( Embedding( max_features, 128))\n",
    "model.add( LSTM( 128, dropout = 0.2, recurrent_dropout = 0.2))\n",
    "model.add( Dense( 1, activation = 'sigmoid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the dense layer uses a sigmoid activation function because there are only two classes to predict - positive or negative. We are almost done, the next step is to compile our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model by setting learning procedure, objective function and evaluation metric\n",
    "model.compile(loss = 'binary_crossentropy', \n",
    "              optimizer = 'adam', \n",
    "              metrics =['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having compiled the model, we fit the model on our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/15\n",
      "25000/25000 [==============================] - 5009s 200ms/step - loss: 0.4717 - acc: 0.7735 - val_loss: 0.3754 - val_acc: 0.8357\n",
      "Epoch 2/15\n",
      "25000/25000 [==============================] - 184s 7ms/step - loss: 0.2998 - acc: 0.8772 - val_loss: 0.3872 - val_acc: 0.8284\n",
      "Epoch 3/15\n",
      "25000/25000 [==============================] - 187s 7ms/step - loss: 0.2117 - acc: 0.9207 - val_loss: 0.3992 - val_acc: 0.8330\n",
      "Epoch 4/15\n",
      "25000/25000 [==============================] - 178s 7ms/step - loss: 0.1468 - acc: 0.9450 - val_loss: 0.4652 - val_acc: 0.8242\n",
      "Epoch 5/15\n",
      "25000/25000 [==============================] - 179s 7ms/step - loss: 0.1101 - acc: 0.9603 - val_loss: 0.5840 - val_acc: 0.8235\n",
      "Epoch 6/15\n",
      "25000/25000 [==============================] - 179s 7ms/step - loss: 0.0746 - acc: 0.9737 - val_loss: 0.6664 - val_acc: 0.8070\n",
      "Epoch 7/15\n",
      "25000/25000 [==============================] - 177s 7ms/step - loss: 0.0635 - acc: 0.9782 - val_loss: 0.6944 - val_acc: 0.8188\n",
      "Epoch 8/15\n",
      "25000/25000 [==============================] - 180s 7ms/step - loss: 0.0430 - acc: 0.9854 - val_loss: 0.8717 - val_acc: 0.8111\n",
      "Epoch 9/15\n",
      "25000/25000 [==============================] - 183s 7ms/step - loss: 0.0330 - acc: 0.9893 - val_loss: 0.8715 - val_acc: 0.8140\n",
      "Epoch 10/15\n",
      "25000/25000 [==============================] - 180s 7ms/step - loss: 0.0258 - acc: 0.9918 - val_loss: 0.9913 - val_acc: 0.8125\n",
      "Epoch 11/15\n",
      "25000/25000 [==============================] - 181s 7ms/step - loss: 0.0284 - acc: 0.9910 - val_loss: 0.8323 - val_acc: 0.8132\n",
      "Epoch 12/15\n",
      "25000/25000 [==============================] - 181s 7ms/step - loss: 0.0178 - acc: 0.9947 - val_loss: 1.0486 - val_acc: 0.8087\n",
      "Epoch 13/15\n",
      "25000/25000 [==============================] - 181s 7ms/step - loss: 0.0136 - acc: 0.9954 - val_loss: 1.2100 - val_acc: 0.8066\n",
      "Epoch 14/15\n",
      "25000/25000 [==============================] - 189s 8ms/step - loss: 0.0131 - acc: 0.9956 - val_loss: 1.1102 - val_acc: 0.8113\n",
      "Epoch 15/15\n",
      "25000/25000 [==============================] - 181s 7ms/step - loss: 0.0095 - acc: 0.9969 - val_loss: 1.2485 - val_acc: 0.8048\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb24716590>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the model \n",
    "model.fit(x_train, y_train, \n",
    "          batch_size = batch_size, \n",
    "          epochs = 15, \n",
    "          validation_data = (x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we evaluate the model on samples stored in the test set which the model has never seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 25s 999us/step\n",
      "('Test score:', 1.248509069707766)\n",
      "('Test accuracy:', 0.80484)\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate( x_test, y_test, batch_size = batch_size) \n",
    "print('Test score:', score) \n",
    "print('Test accuracy:', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
